{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xABx3A-soZMM"
      },
      "source": [
        "# Aprendizaje Profundo para Procesamiento deSeñales de Imagen y Vídeo (APPIV)\n",
        "## Master in Data Science / Máster en Ciencia de Datos\n",
        "## Universidad Autonoma de Madrid\n",
        "\n",
        "## **Tutorial: multiple object tracking**\n",
        "\n",
        "This tutorial will show you **how to define and run a simple multi-object tracker**. This tracker is based on the tracking-by-detection scheme. Hence, the first step is an object detector which is applied to each frame independently and the second step is data association where the detections are linked to the tracks of the previous frame.\n",
        "\n",
        "For evaluation, you will use the [MOT16](https://motchallenge.net/data/MOT16/) dataset.  To compare tracking performance of different trackers without the effect of the object detector, the [MOTChallenge](https://motchallenge.net/data/MOT16/) provides precomputed sets of public object detections. Trackers are then evaluated on their capabilities to form tracks with the provided set. However, this tutorial uses detections computed with your own detector (`private detections`).\n",
        "\n",
        "This tutorial can be run using cloud resources via `Google Colaboratory` or using local resources (i.e. lab computers) using Jupyter notebooks and through `Colab` (see for example this [link](https://research.google.com/colaboratory/local-runtimes.html#:~:text=In%20Colaboratory%2C%20click%20the%20%22Connect,connected%20to%20your%20local%20runtime))\n",
        "\n",
        "Author: Juan Carlos San Miguel (juancarlos.sanmiguel@uam.es)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFYSSMiwpxSq"
      },
      "source": [
        "# 1 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZpIZY_8r9NT"
      },
      "source": [
        "## 1.1 Install Python libraries\n",
        "\n",
        "The following code will install some required packages and the utilities to compute the performance evaluation metrics `py-motmetrics` (an alternative link is available in case the official one does not work).\n",
        "\n",
        "This tutorial has been tested in `Google Colaboratory` for the following versions:\n",
        "* `Python` 3.11.X\n",
        "* `Pytorch` 2.6.0 with `torchvision` 0.23.0 and `torchaudio` 2.6.0\n",
        "* `CUDA libraries` 12.4\n",
        "* `Pandas` 2.2.2\n",
        "* `motmetrics` 1.4.0\n",
        "\n",
        "At your own risk, feel free to employ other versions for the required packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRMsynpFU6gh"
      },
      "source": [
        "!pip3 install tqdm lap pandas==2.2.2\n",
        "!pip3 install torch==2.6.0+cu124 torchvision==0.21.0+cu124 torchaudio===2.6.0+cu124 --extra-index-url https://download.pytorch.org/whl/cu124 #Pytorch and requirements\n",
        "!pip3 install git+https://github.com/jcsma-utils-teaching/py-motmetrics.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check versions installed"
      ],
      "metadata": {
        "id": "uzyjziFJN-u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision, torchaudio, motmetrics, pandas\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(motmetrics.__version__)\n",
        "print(pandas.__version__)"
      ],
      "metadata": {
        "id": "I8qUgw871bIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVbxCOi7bhYl"
      },
      "source": [
        "## 1.2 Connect to Google Drive\n",
        "We will connect to your Google Drive account so we can save downloaded data and obtained results.\n",
        "\n",
        "If you are not using `Google Colaboratory` (e.g. running the notebook in computer labs), you do not need to execute the following code and also, you may need changing the paths indicated by `download_dir` and `working_dir` to suitable locations adapted to your setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ILTZaDoGQbC"
      },
      "source": [
        "# Connet to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GQNpPCBKqmd"
      },
      "source": [
        "## 1.3 Download material for the assignment\n",
        "\n",
        "We will download the material for the assignment in [this zip file](http://www-vpu.eps.uam.es/~jcs/APPIV/appiv_p2_material.zip) to your Google Drive account.\n",
        "\n",
        "The `download_dir` path points to the downloads directory in your Google Drive. You need to download this file once.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QmwLco3ACon"
      },
      "source": [
        "import os, subprocess\n",
        "\n",
        "# path to download in Google Drive\n",
        "download_dir = '/content/gdrive/My Drive/downloads/'\n",
        "\n",
        "# file with the material for the assignment\n",
        "urlfile = 'http://www-vpu.eps.uam.es/~jcs/teaching/APPSIV/appsiv_p2_material.zip'\n",
        "datafile = os.path.basename(urlfile) #with extension\n",
        "\n",
        "# Download material if not available in download directory\n",
        "datadownload_path = os.path.join(download_dir,datafile)\n",
        "if os.path.isfile(datadownload_path) == False:\n",
        "  !curl $urlfile --create-dirs -o \"$datadownload_path\"\n",
        "  print('The file \\'{}\\' has been downloaded to directory \\'{}\\''.format(datafile,download_dir))\n",
        "else:\n",
        "  print('The file \\'{}\\' is already available in the directory \\'{}\\''.format(datafile,download_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2T6bh9FK0ct"
      },
      "source": [
        "## 1.4 Copy material to local directory\n",
        "\n",
        "We will copy the downloaded material to a local directory of the Virtual Machine (VM), as the processing/reading is faster than using Drive folders. The `working_dir` path points to a working directory in the Virtual Machine .\n",
        "\n",
        "Data (dataset and source code files) is copied from the `download_dir` directory to the `working_dir` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHW7s7czD_rd"
      },
      "source": [
        "import os, subprocess, zipfile\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "# path to working directory (local, will be deleted when VM disconnected)\n",
        "working_dir = '/content/work/'\n",
        "\n",
        "# copy downloaded file to the working directory\n",
        "print('Copying file \\'{}\\' from \\'{}\\' to \\'{}\\'...'.format(datafile,download_dir,working_dir))\n",
        "if os.path.isfile(os.path.join(working_dir,datafile)) == False:\n",
        "  !rsync -ah --progress \"$datadownload_path\" \"$working_dir\"\n",
        "else:\n",
        "  print('already exists!')\n",
        "\n",
        "# uncompress/unzip file in the working directory\n",
        "print('\\nUnzipping file \\'{}\\' in directory \\'{}\\'...'.format(datafile, working_dir), end='')\n",
        "if os.path.isdir(os.path.join(working_dir,'data')) == False:\n",
        "  with zipfile.ZipFile(os.path.join(working_dir,datafile)) as zf:\n",
        "    for member in tqdm(zf.infolist(), desc='Extracting '):\n",
        "        try:\n",
        "          zf.extract(member, working_dir)\n",
        "        except zipfile.error as e:\n",
        "            pass\n",
        "else:\n",
        "  print('already unzipped!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wme28GdFAFY"
      },
      "source": [
        "##1.5 Add source code location to system path\n",
        "\n",
        "We will add the path of the source code provided in the material, so we can import and run the required functionalities for this assignment (tracker, metrics,...).\n",
        "\n",
        "If everything has been setup correctly, the downloaded source code should be in the directory '/content/work/src/'\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmWYO8SeLVTp"
      },
      "source": [
        "import sys,os\n",
        "\n",
        "# add the path to the system so we can import the tracker\n",
        "sys.path.append(os.path.join(working_dir,'src/'))\n",
        "\n",
        "# test that we can successfully import the tracker\n",
        "from tracker.tracker import Tracker\n",
        "print('If \\'from tracker.tracker import Tracker\\' shows no errors, you can import and execute the sample tracker code.\\n')\n",
        "print('Source files of the tracker are available at the directory \\'{}\\':'.format(working_dir+'src/tracker/'))\n",
        "!ls $working_dir/src/tracker\n",
        "\n",
        "print('object_detector.py --> defines the object detector to be employed for tracking (FRCNN_FPN)')\n",
        "print('tracker.py         --> defines the architecture for developing trackers (i.e. common functions)')\n",
        "print('data_obj_detect.py --> utility for handing datasets/sequences for object detection')\n",
        "print('data_track.py      --> utility for handing datasets/sequences for object tracking')\n",
        "print('utils.py           --> Utilities for evaluation and plotting')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Qe2615tOQQ"
      },
      "source": [
        "# 2 Dataset: MOT16\n",
        "\n",
        "The MOT16 challenge provides 7 train and 7 test video sequences with multiple objects (pedestrians) per frame. It includes many challening scenarios with camera movement, high crowdedness and object occlusions. See the [webpage](https://motchallenge.net/data/MOT16/) for video sequences with ground truth annotation. For the `train` set, ground-truth data and images are provided, so you can train and evaluate your algorithm. For the `test` set, only images are provided and ground-truth data is unknown to researchers. The evaluation is carried out using an evaluation server at the [MOT challenge site](https://motchallenge.net/instructions/). This evaluation scheme prevents cheating by doing training also on the `test` set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgOJGKlblNqH"
      },
      "source": [
        "##2.1 Train/test split\n",
        "\n",
        "For this assignment, we have splitted the original `train` set of MOT16 in two sets for `train` (4 sequences) and `test` (3 sequences). The following code shows the splitting done in the downloaded material:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcbme23UQ5p2"
      },
      "source": [
        "#list the contents of the 'train' directory\n",
        "train_dir = os.path.join(working_dir,'data/MOT16/train')\n",
        "print('Train directory:')\n",
        "!ls $train_dir\n",
        "\n",
        "#list the contents of the 'test' directory\n",
        "test_dir = os.path.join(working_dir,'data/MOT16/test')\n",
        "print('Test directory:')\n",
        "!ls $test_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KEDZJVwQ6DX"
      },
      "source": [
        "## 2.2 Visualization\n",
        "\n",
        "The following code shows the ground-truth detections for each sequence in the `train` and `test` sets.\n",
        "\n",
        "The `MOT16Sequences` dataset class provides the possibilty to load single sequences, e.g., `seq_name = 'mot16_02'`, or the entire train/test set, e.g., `seq_name = 'mot16_train'`. The associated code is defined in the file `./src/tracker/data_track.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rDoJXYBr0FL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tracker.data_track import MOT16Sequences\n",
        "\n",
        "seq_name = 'MOT16-02'\n",
        "#seq_name = 'MOT16-train'\n",
        "seq_name = 'MOT16-test'\n",
        "data_dir = os.path.join(working_dir,'data/MOT16')\n",
        "sequences = MOT16Sequences(seq_name, data_dir, load_seg=True)\n",
        "\n",
        "for seq in sequences:\n",
        "    for i, frame in enumerate(seq):\n",
        "        img = frame['img']\n",
        "\n",
        "        dpi = 96\n",
        "        fig, ax = plt.subplots(1, dpi=dpi)\n",
        "\n",
        "        img = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
        "        width, height, _ = img.shape\n",
        "\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        fig.set_size_inches(width / dpi, height / dpi)\n",
        "\n",
        "        if 'gt' in frame:\n",
        "            gt = frame['gt']\n",
        "            for gt_id, box in gt.items():\n",
        "                rect = plt.Rectangle(\n",
        "                  (box[0], box[1]), box[2] - box[0],box[3] - box[1],\n",
        "                  fill=False,\n",
        "                  edgecolor=\"red\",\n",
        "                  linewidth=1.0)\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        break #we only plot the first image for each sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cvaNe42gIll"
      },
      "source": [
        "#3 Object detector\n",
        "\n",
        "We provide you with an object detector pretrained on the MOT challenge training set. This detector can be used and improved to generate the framewise detections necessary for the subsequent tracking and data association step.\n",
        "\n",
        "The object detector is a [Faster R-CNN](https://arxiv.org/abs/1506.01497) with a Resnet50 feature extractor. We trained the native PyTorch implementation of Faster-RCNN for pedestrian detection. For more information check out the corresponding PyTorch [tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
        "\n",
        "The associated code is defined in the file `./src/tracker/object_detector.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AJXq06_yMmX"
      },
      "source": [
        "## 3.1 Model\n",
        "\n",
        "The trained model is provided in the directory `model_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzFdHZY_6DLf"
      },
      "source": [
        "# path for the source code of the tracker\n",
        "model_dir=working_dir+'models/'\n",
        "\n",
        "print('Model files of the tracker are available at the directory \\'{}\\':'.format(model_dir))\n",
        "!ls \"$model_dir\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qubi7uE6EPd"
      },
      "source": [
        "##3.2 Configuration\n",
        "The configuration of the detector is described as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2g7oHASTa8v"
      },
      "source": [
        "import torch\n",
        "\n",
        "#select GPU if available\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "#location of the model file\n",
        "obj_detect_model_file = os.path.join(working_dir, \"models/faster_rcnn_fpn.model\")\n",
        "\n",
        "#threshold for non maximum suppression\n",
        "obj_detect_nms_thresh = 0.3\n",
        "\n",
        "#detector has been trained for two classes\n",
        "num_classes=2 # 1 class (person) + background (see https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mo46L8dzys1"
      },
      "source": [
        "##3.3 Creation of the object detector\n",
        "\n",
        "To create the object detector, we will use the functionality provided in the file `./src/tracker/object_detector.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIXdzYMnMhiW"
      },
      "source": [
        "from tracker.object_detector import FRCNN_FPN\n",
        "\n",
        "# object detector\n",
        "obj_detect = FRCNN_FPN(num_classes=num_classes, nms_thresh=obj_detect_nms_thresh)\n",
        "obj_detect_state_dict = torch.load(obj_detect_model_file,map_location=lambda storage, loc: storage)\n",
        "obj_detect.load_state_dict(obj_detect_state_dict)\n",
        "\n",
        "# prints the architecture and sets the model to evaluation mode.\n",
        "obj_detect.eval()\n",
        "\n",
        "# loads detector to CPU or GPU (if available)\n",
        "obj_detect.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3B32wE0o0q"
      },
      "source": [
        "##3.4 Evaluate the object detector\n",
        "\n",
        "In addition, the trained object detector can be evaluated on the `MOT16` dataset.\n",
        "To load the  MOT16 data (images and ground-truth bounding boxes), we will use functionality provide by MOT16 dev kit (`./src/tracker/data_obj_detect.py`) and Pytorch (`Dataloader`).\n",
        "\n",
        "To perform such evaluation, we will use functionalities provided in the file `./src/tracker/utils.py`. If you run the following code, you will get an evaluation of the object detector over the training set. Estimated execution and testing time (15-20 minutes for the train set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc6EXVCyBhtV"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tracker.data_obj_detect import MOT16ObjDetect\n",
        "from tracker.utils import (evaluate_obj_detect, obj_detect_transforms)\n",
        "\n",
        "#load train set for the MOT16 data (images and ground-truth bounding boxes)\n",
        "dataset_test = MOT16ObjDetect(os.path.join(working_dir, 'data/MOT16/train'), obj_detect_transforms(train=False))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=2,collate_fn=collate_fn)\n",
        "\n",
        "#evaluation\n",
        "evaluate_obj_detect(obj_detect, data_loader_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGszq40utUfA"
      },
      "source": [
        "#4 Multi-object tracking\n",
        "\n",
        "We provide a simple baseline tracker which predicts object detections for each frame and generates tracks by assigning current detections to previous detections via Intersection over Union.\n",
        "\n",
        "Your task is to work using this baseline, trying to improve the performance (`MOTA` metric)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5QMs7Yi4Wol"
      },
      "source": [
        "## 4.1 Configuration\n",
        "As configuration we need to define the object detector, the sequences to be analyzed and the output directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQTAqnFptwul"
      },
      "source": [
        "import torch\n",
        "from tracker.object_detector import FRCNN_FPN\n",
        "from tracker.data_track import MOT16Sequences\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# object detector\n",
        "obj_detect_model_file = os.path.join(working_dir, \"models/faster_rcnn_fpn.model\") #location of the model file\n",
        "obj_detect_nms_thresh = 0.3 #threshold for non maximum suppression\n",
        "num_classes=2 # 1 class (person) + background (see https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
        "obj_detect = FRCNN_FPN(num_classes=num_classes, nms_thresh=obj_detect_nms_thresh)\n",
        "obj_detect_state_dict = torch.load(obj_detect_model_file,map_location=lambda storage, loc: storage)\n",
        "obj_detect.load_state_dict(obj_detect_state_dict)\n",
        "obj_detect.eval()     # set to evaluation mode\n",
        "obj_detect.to(device) # load detector to GPU or CPU\n",
        "\n",
        "# select dataset\n",
        "seq_name = 'MOT16-test' #'MOT16-train', 'MOT16-02'\n",
        "data_dir = os.path.join(working_dir, 'data/MOT16')\n",
        "sequences = MOT16Sequences(seq_name, data_dir)\n",
        "print('Loaded {:d} sequences for {:s}'.format(len(sequences),seq_name))\n",
        "\n",
        "#output directory\n",
        "output_dir = os.path.join(working_dir, 'output')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EuukzMtl9wD"
      },
      "source": [
        "Also, run the following code to allow repeatable results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mREBcQ4DmDS-"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "seed = 12345 #seed to allow repeatable results\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5KgKxhmMm1r"
      },
      "source": [
        "## 4.2 Extend the baseline tracker\n",
        "\n",
        "The class `Tracker` in file `./src/tracker/tracker.py` contains the skeleton of the baseline tracker. You modifications and improvements should be based on this class.\n",
        "\n",
        "As a suggestion, we provide a code example to extend the functionality of the baseline tracker by overriding the `data_association` function (which is empty in the baseline tracker). This extensions generates tracks by assigning current detections to previous detections via `Intersection over Union` in a greedy way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV2c5yengHyC"
      },
      "source": [
        "import numpy as np\n",
        "import motmetrics as mm\n",
        "mm.lap.default_solver = 'lap'\n",
        "from tracker.tracker import Tracker\n",
        "\n",
        "# extended class 'Tracker' based on IoU criterion\n",
        "class TrackerIoUAssignment(Tracker):\n",
        "\n",
        "    def data_association(self, boxes, scores):\n",
        "        if self.tracks:\n",
        "            track_ids = [t.id for t in self.tracks]\n",
        "            track_boxes = np.stack([t.box.numpy() for t in self.tracks], axis=0)\n",
        "\n",
        "            # compute distance based on IoU (distance=1-IoU)\n",
        "            distance = mm.distances.iou_matrix(track_boxes, boxes.numpy(), max_iou=0.5)\n",
        "\n",
        "            # update existing tracks\n",
        "            remove_track_ids = []\n",
        "            for t, dist in zip(self.tracks, distance):\n",
        "                if np.isnan(dist).all():\n",
        "                    remove_track_ids.append(t.id)\n",
        "                else:\n",
        "                    match_id = np.nanargmin(dist)\n",
        "                    t.box = boxes[match_id]\n",
        "            self.tracks = [t for t in self.tracks\n",
        "                           if t.id not in remove_track_ids]\n",
        "\n",
        "            # add new tracks\n",
        "            new_boxes = []\n",
        "            new_scores = []\n",
        "            for i, dist in enumerate(np.transpose(distance)):\n",
        "                if np.isnan(dist).all():\n",
        "                    new_boxes.append(boxes[i])\n",
        "                    new_scores.append(scores[i])\n",
        "            self.add(new_boxes, new_scores)\n",
        "\n",
        "        else:\n",
        "            self.add(boxes, scores)\n",
        "\n",
        "# create a new tracker using the 'TrackerIoUAssignment' class\n",
        "tracker = TrackerIoUAssignment(obj_detect)\n",
        "print('Tracker created!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvB1hRhWtwaE"
      },
      "source": [
        "## 4.3 Run tracker\n",
        "The following code runs the tracker created previously for the selected test sequences. It also prints some statistics for each sequence.\n",
        "\n",
        "The estimated running time for the `train`/`test` sets is 10-15 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG3QwqUXtVu2"
      },
      "source": [
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.autonotebook import tqdm\n",
        "from tracker.utils import get_mot_accum\n",
        "\n",
        "time_total = 0\n",
        "mot_accums = []\n",
        "results_seq = {}\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Tracking: {seq}\")\n",
        "    now = time.time()\n",
        "\n",
        "    # restart tracker state for each sequence\n",
        "    tracker.reset()\n",
        "\n",
        "    #load data\n",
        "    data_loader = DataLoader(seq, batch_size=1, shuffle=False)\n",
        "\n",
        "    #run tracker\n",
        "    for frame in tqdm(data_loader):\n",
        "        tracker.step(frame)\n",
        "\n",
        "    #keep results\n",
        "    results = tracker.get_results()\n",
        "    results_seq[str(seq)] = results\n",
        "\n",
        "    #perform evaluation\n",
        "    if seq.no_gt:\n",
        "        print(f\"No GT evaluation data available.\")\n",
        "    else:\n",
        "        mot_accums.append(get_mot_accum(results, seq)) #compute and store eval metrics\n",
        "\n",
        "    time_total += time.time() - now\n",
        "\n",
        "    print(f\"Tracks found: {len(results)}\")\n",
        "    print(f\"Runtime for {seq}: {time.time() - now:.1f} s.\")\n",
        "\n",
        "    #save results to output directory\n",
        "    seq.write_results(results, os.path.join(output_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEFOqB2q4h9s"
      },
      "source": [
        "## 4.4 Evaluate tracking results\n",
        "\n",
        "The following code computes the performance evaluation metrics for the generated tracking results. You can check the definitions of the evaluation metrics at this [paper](https://arxiv.org/abs/1603.00831)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1YWFTzC4gib"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "from tracker.utils import evaluate_mot_accums\n",
        "\n",
        "print(f\"Runtime for all sequences: {time_total:.1f} s.\")\n",
        "if mot_accums:\n",
        "    evaluate_mot_accums(mot_accums,\n",
        "                        [str(s) for s in sequences if not s.no_gt],\n",
        "                        generate_overall=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR5oU3c-LeWg"
      },
      "source": [
        "As a reference, the current state-of-the-art multi-object tracker [Tracktor++](https://arxiv.org/abs/1903.05625) achieves the following tracking results on the full `MOT16-train` sequences:\n",
        "\n",
        "              IDF1   IDP   IDR  Rcll  Prcn  GT  MT  PT  ML  FP    FN IDs   FM  MOTA  MOTP\n",
        "    MOT16-02 45.8% 78.3% 32.4% 41.3% 99.8%  62   9  32  21  18 10909  59   68 40.9% 0.080\n",
        "    MOT16-04 71.1% 90.3% 58.6% 64.7% 99.8%  83  32  29  22  71 16785  22   29 64.5% 0.096\n",
        "    MOT16-05 64.0% 86.6% 50.7% 57.5% 98.1% 133  32  65  36  75  2942  37   59 55.8% 0.144\n",
        "    MOT16-09 54.6% 69.4% 45.0% 64.3% 99.1%  26  11  13   2  31  1903  22   31 63.3% 0.086\n",
        "    MOT16-10 64.3% 75.7% 55.9% 72.4% 98.0%  57  28  26   3 189  3543  71  125 70.4% 0.148\n",
        "    MOT16-11 63.3% 77.0% 53.7% 69.0% 98.9%  75  24  33  18  73  2924  26   26 68.0% 0.081\n",
        "    MOT16-13 73.6% 85.1% 64.8% 74.2% 97.6% 110  60  39  11 213  3000  62   90 71.9% 0.132\n",
        "    OVERALL  65.0% 84.0% 53.1% 62.6% 99.1% 546 196 237 113 670 42006 299  428 61.7% 0.106\n",
        "\n",
        "Note that the `MOT16-train` set is different from the sets of this assignment:  `train` (first four sequences of `MOT16-train`) and `test` (last three sequences of `MOT16-train`).\n",
        "\n",
        "For your final submission you should focus on improving `MOTA` for the `train` and `test` sets provided as material of this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7KPPxqG_5xt"
      },
      "source": [
        "## 4.5 Visualize tracking results\n",
        "\n",
        "The following code shows some sample results for a particular sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq6tAiUBkBEk"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tracker.utils import plot_sequence\n",
        "\n",
        "plot_sequence(results_seq['MOT16-11'],\n",
        "              [s for s in sequences if str(s) == 'MOT16-11'][0],\n",
        "              first_n_frames=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPHE1pTfYPAw"
      },
      "source": [
        "## 4.6 Copy results to Google Drive\n",
        "The `seq.write_results(results, os.path.join(output_dir))` statement saves predicted tracks into files. After executing this cell the `output` directory in your Google Drive should contain multiple `MOT16-XY.txt` files.\n",
        "\n",
        "Here we will copy these results in the `download_dir` directory of your Google Drive account."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46i1oouMYUoX"
      },
      "source": [
        "#copy output files to download directory in personal Google Drive\n",
        "!cp -r $output_dir \"$download_dir\"\n",
        "\n",
        "#show output files\n",
        "print('Output tracking results:')\n",
        "path = os.path.join(download_dir,\"output\")\n",
        "!ls \"$path\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpyoLBQWnBBV"
      },
      "source": [
        "**Remember you will have to submit these files for both the `train` and `test` sets (in addition to the code for running your tracker).**"
      ]
    }
  ]
}